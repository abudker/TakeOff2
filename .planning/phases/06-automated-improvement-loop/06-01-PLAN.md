---
phase: 06-automated-improvement-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/improvement/loop.py
  - src/improvement/history.py
autonomous: true

must_haves:
  truths:
    - "ImprovementLoop.run() executes iterations until a stop condition is met"
    - "ImprovementLoop.should_stop() returns True when target F1 reached, max iterations hit, or plateau detected"
    - "IterationHistory persists iteration metrics to JSON and can be loaded on resume"
    - "Plateau detection correctly identifies no-improvement windows using patience and min_delta"
    - "Loop can resume from a specific iteration by loading truncated history"
  artifacts:
    - path: "src/improvement/loop.py"
      provides: "ImprovementLoop class with run(), should_stop(), _run_iteration()"
      min_lines: 120
    - path: "src/improvement/history.py"
      provides: "IterationHistory class with add_iteration(), get_best_iteration(), save/load"
      min_lines: 80
  key_links:
    - from: "src/improvement/loop.py"
      to: "src/improvement/history.py"
      via: "ImprovementLoop uses IterationHistory for state persistence"
      pattern: "IterationHistory"
    - from: "src/improvement/loop.py"
      to: "src/improvement/cli.py"
      via: "Loop calls existing run_extraction(), run_verification(), invoke_critic(), apply_proposal()"
      pattern: "run_extraction|run_verification|invoke_critic|apply_proposal"
---

<objective>
Create the loop controller and iteration history modules that power the automated improvement loop.

Purpose: These two modules are the core engine of Phase 6 -- the loop controller orchestrates iterative improve cycles (critic -> apply -> extract -> verify) with configurable stop conditions (target F1, max iterations, plateau detection), while the history module provides JSON-based persistence for metrics tracking and resume capability.

Output: Two new Python modules in src/improvement/ that can be imported by the CLI (Plan 02).
</objective>

<execution_context>
@/Users/Andrew/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Andrew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automated-improvement-loop/06-RESEARCH.md
@.planning/phases/05-manual-improvement-loop/05-03-SUMMARY.md

Key existing code to reference:
@src/improvement/cli.py - Contains run_extraction(), run_verification(), git_commit_iteration(), load_aggregate_metrics(), get_next_iteration(), get_eval_ids() -- all reusable from loop
@src/improvement/critic.py - Contains invoke_critic(), parse_proposal(), aggregate_failure_analysis(), load_eval_results(), InstructionProposal
@src/improvement/apply.py - Contains apply_proposal()
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create IterationHistory module</name>
  <files>src/improvement/history.py</files>
  <action>
Create `src/improvement/history.py` with an IterationHistory class for JSON-based persistence of iteration metrics.

**Class: IterationHistory**
- `__init__(self, history_file: Path)` - Load existing history from JSON file if it exists
- `save(self)` - Write iterations list + last_updated timestamp to JSON
- `add_iteration(self, iteration_num: int, metrics: dict, proposal_summary: Optional[dict], status: str = "success")` - Append iteration record and auto-save. Each record has: iteration, timestamp (ISO), metrics (f1, precision, recall, errors_by_type), proposal_summary (target_file, change_type, hypothesis truncated to 100 chars), status
- `get_latest_metrics(self) -> Optional[dict]` - Return metrics from most recent iteration
- `get_best_iteration(self) -> Optional[dict]` - Return iteration dict with highest F1
- `get_f1_progression(self) -> List[float]` - Return list of F1 values in order
- `truncate_to(self, iteration: int)` - Remove all iterations after given number (for resume from checkpoint)
- `__len__(self)` - Return number of iterations

**JSON schema:**
```json
{
  "iterations": [
    {
      "iteration": 1,
      "timestamp": "2026-02-06T...",
      "metrics": {"f1": 0.815, "precision": 0.82, "recall": 0.81, "errors_by_type": {...}},
      "proposal_summary": {"target_file": "...", "change_type": "...", "hypothesis": "..."},
      "status": "success"
    }
  ],
  "last_updated": "2026-02-06T..."
}
```

History file location: `evals/.improvement-history.json` (passed in by caller, not hardcoded).

Use `json` stdlib for persistence, `datetime` for timestamps, `pathlib` for paths. No external dependencies.
  </action>
  <verify>
Run: `python3 -c "from improvement.history import IterationHistory; print('OK')"` from project root.
Verify the class can be instantiated with a temp file path and add_iteration/save/load round-trips correctly.
  </verify>
  <done>IterationHistory class exists, can persist iterations to JSON, load on init, track F1 progression, find best iteration, and truncate for resume.</done>
</task>

<task type="auto">
  <name>Task 2: Create ImprovementLoop controller</name>
  <files>src/improvement/loop.py</files>
  <action>
Create `src/improvement/loop.py` with an ImprovementLoop class that orchestrates automated improvement iterations.

**Class: ImprovementLoop**

Constructor parameters:
- `evals_dir: Path` - Path to evals directory
- `max_iterations: int = 50` - Safety valve
- `target_f1: float = 0.90` - Target metric
- `plateau_patience: int = 5` - Iterations without improvement before stopping
- `min_improvement: float = 0.005` - Minimum F1 delta to count as improvement
- `focus: Optional[str] = None` - Focus agent for critic
- `focus_reason: Optional[str] = None` - Why focusing
- `no_commit: bool = False` - Skip git commits
- `consecutive_failure_limit: int = 3` - Stop after N consecutive failures (critic fails, no proposal, subprocess error)

Instance variables:
- `self.history: IterationHistory` - loaded from `evals_dir.parent / ".improvement-history.json"`
- `self.project_root: Path` = evals_dir.parent
- `self.consecutive_failures: int = 0`

**Methods:**

`should_stop(self) -> Tuple[bool, str]`:
- Check target F1 achieved (>= target_f1)
- Check max iterations reached
- Check plateau: look at last `plateau_patience` iterations' F1 values. If `max(recent) - min(recent) < min_improvement`, it's a plateau
- Check consecutive failures >= limit
- Return (should_stop, reason_string)

`_run_iteration(self) -> Tuple[bool, Optional[dict]]`:
This runs ONE improvement cycle. Reuse the existing functions from cli.py and critic.py:

1. Load eval results: `load_eval_results(evals_dir, eval_ids)`
2. Get before metrics: `load_aggregate_metrics(evals_dir)`
3. Aggregate failure analysis: `aggregate_failure_analysis(results)`
4. Invoke critic: `invoke_critic(analysis, instructions_dir, project_root, focus_agent=focus, focus_reason=focus_reason)` -- wrapped in try/except, return (False, None) on failure
5. Parse proposal: `parse_proposal(critic_output)` -- if None, treat as failure (no valid proposal)
6. Apply proposal: `apply_proposal(proposal, project_root, iteration_dirs)` using `get_next_iteration()` for iteration number
7. Run extraction: `run_extraction(evals_dir)` -- return (False, None) on failure
8. Run verification: `run_verification(evals_dir)` -- return (False, None) on failure
9. Get after metrics: `load_aggregate_metrics(evals_dir)`
10. Optionally git commit: `git_commit_iteration(proposal, before_metrics, after_metrics, iteration, project_root)`
11. Return (True, after_metrics)

Import all these from existing modules (cli.py, critic.py, apply.py). Do NOT duplicate their implementations.

`run(self) -> dict`:
Main loop. Uses Rich Console for output (NOT Rich Progress -- simple console prints are clearer for long-running improvement loops where each iteration takes minutes).

```
console.print header (target, max_iter, patience)
while True:
    should_stop, reason = self.should_stop()
    if should_stop: break

    iteration_num = len(self.history) + 1
    console.print iteration header

    success, metrics = self._run_iteration()

    if success:
        self.consecutive_failures = 0
        self.history.add_iteration(iteration_num, metrics, proposal_summary, "success")
        console.print F1 value and delta from previous
    else:
        self.consecutive_failures += 1
        self.history.add_iteration(iteration_num, before_metrics_or_empty, None, "failed")
        console.print failure message

console.print summary (iterations run, best F1, final F1, stop reason)
return summary dict
```

`resume(self, from_iteration: int)`:
- Truncate history to from_iteration
- Call self.run()

**Important implementation notes:**
- Import from existing modules: `from .cli import run_extraction, run_verification, git_commit_iteration, load_aggregate_metrics, get_next_iteration, get_eval_ids`
- Import from critic: `from .critic import load_eval_results, aggregate_failure_analysis, invoke_critic, parse_proposal`
- Import from apply: `from .apply import apply_proposal`
- Handle KeyboardInterrupt gracefully in run() -- save history, print summary, re-raise
- Use `from rich.console import Console` for output, not click.echo (Rich handles styling better for long output)
  </action>
  <verify>
Run: `python3 -c "from improvement.loop import ImprovementLoop; print('OK')"` from project root.
Verify the class can be instantiated with evals_dir=Path("evals").
  </verify>
  <done>ImprovementLoop class exists with run(), should_stop(), _run_iteration(), and resume() methods. It reuses all existing improvement infrastructure and adds loop control + history tracking.</done>
</task>

</tasks>

<verification>
1. Both modules import without errors: `python3 -c "from improvement.loop import ImprovementLoop; from improvement.history import IterationHistory; print('All imports OK')"`
2. IterationHistory round-trips: create instance, add_iteration, verify save file exists, create new instance from same file, verify iterations loaded
3. ImprovementLoop.should_stop() returns correct results for: empty history (False), target achieved (True), max iterations (True), plateau (True)
</verification>

<success_criteria>
- src/improvement/loop.py exists with ImprovementLoop class (~150-200 lines)
- src/improvement/history.py exists with IterationHistory class (~80-120 lines)
- Both import cleanly without errors
- Loop controller reuses existing cli.py/critic.py/apply.py functions (no duplication)
- Stop conditions cover: target F1, max iterations, plateau detection, consecutive failures
- History persists to JSON and supports resume via truncation
</success_criteria>

<output>
After completion, create `.planning/phases/06-automated-improvement-loop/06-01-SUMMARY.md`
</output>
