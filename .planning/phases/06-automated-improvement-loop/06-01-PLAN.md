---
phase: 06-automated-improvement-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/improvement/history.py
  - src/improvement/loop.py
autonomous: true

must_haves:
  truths:
    - "Loop can track iteration metrics across multiple runs"
    - "Loop detects when target F1 is achieved"
    - "Loop detects plateau (no improvement for K iterations)"
    - "Loop stops at max iterations"
  artifacts:
    - path: "src/improvement/history.py"
      provides: "Iteration history persistence and querying"
      exports: ["IterationHistory"]
      contains: "class IterationHistory"
    - path: "src/improvement/loop.py"
      provides: "Loop controller with stop conditions"
      exports: ["ImprovementLoop", "StopReason"]
      contains: "class ImprovementLoop"
  key_links:
    - from: "src/improvement/loop.py"
      to: "src/improvement/history.py"
      via: "IterationHistory import"
      pattern: "from .history import IterationHistory"
    - from: "src/improvement/loop.py"
      to: "src/improvement/cli.py"
      via: "run_extraction, run_verification functions"
      pattern: "from .cli import.*run_extraction"
---

<objective>
Create loop controller and iteration history modules for automated improvement loop

Purpose: Enable automated execution of improvement iterations with intelligent stop conditions (target achieved, plateau detected, max iterations reached) and persistent history tracking for resume capability.

Output: Two new modules - history.py for iteration persistence and loop.py for loop orchestration with stop conditions.
</objective>

<execution_context>
@/Users/Andrew/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Andrew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-automated-improvement-loop/06-RESEARCH.md
@src/improvement/cli.py
@src/improvement/critic.py
@src/improvement/apply.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Iteration History Module</name>
  <files>src/improvement/history.py</files>
  <action>
Create IterationHistory class for tracking improvement loop state with JSON persistence.

**Class: IterationHistory**
```python
@dataclass
class IterationRecord:
    iteration: int
    timestamp: str  # ISO format
    metrics: dict   # {f1, precision, recall, errors_by_type}
    proposal: Optional[dict]  # Proposal that was applied (if any)
    status: str  # "success", "failed", "skipped"
    changes_made: Optional[str]  # Summary of what changed

class IterationHistory:
    def __init__(self, history_file: Path)
    def _load(self)  # Load from JSON file
    def save(self)   # Persist to JSON file
    def add_iteration(iteration_num, metrics, proposal, status, changes_made)
    def get_latest_metrics() -> Optional[dict]
    def get_best_iteration() -> Optional[IterationRecord]
    def get_f1_progression() -> List[float]  # For plotting/display
    def get_iterations_since_improvement(min_delta: float = 0.001) -> int  # For plateau detection
    def truncate_to(iteration: int)  # For resume from specific iteration
```

**File format (.improvement-history.json):**
```json
{
  "iterations": [
    {
      "iteration": 1,
      "timestamp": "2026-02-04T10:00:00",
      "metrics": {"f1": 0.15, "precision": 0.20, "recall": 0.12},
      "proposal": {"target_file": "...", "change_type": "..."},
      "status": "success",
      "changes_made": "Added wall extraction guidance"
    }
  ],
  "last_updated": "2026-02-04T10:05:00",
  "baseline_metrics": {"f1": 0.069, ...}  # Initial metrics before loop started
}
```

**Key behaviors:**
- History file stored at project root: `.improvement-history.json`
- Save after every iteration (immediate persistence)
- Preserve baseline_metrics from first load (before any proposals applied)
- get_iterations_since_improvement counts iterations since last F1 improvement > min_delta
  </action>
  <verify>
```python
# Test in Python REPL
from pathlib import Path
import sys; sys.path.insert(0, 'src')
from improvement.history import IterationHistory, IterationRecord

# Create test history
h = IterationHistory(Path("/tmp/test-history.json"))
h.add_iteration(1, {"f1": 0.1, "precision": 0.1, "recall": 0.1}, None, "success", "test")
h.add_iteration(2, {"f1": 0.12, "precision": 0.12, "recall": 0.12}, None, "success", "test2")
assert h.get_latest_metrics()["f1"] == 0.12
assert h.get_best_iteration().iteration == 2
assert len(h.get_f1_progression()) == 2
print("History module tests pass")
```
  </verify>
  <done>IterationHistory class persists iteration metrics to JSON, supports querying best iteration and F1 progression, truncate_to enables resume from specific iteration</done>
</task>

<task type="auto">
  <name>Task 2: Create Loop Controller Module</name>
  <files>src/improvement/loop.py</files>
  <action>
Create ImprovementLoop class that orchestrates automated improvement iterations with stop conditions.

**Enum: StopReason**
```python
class StopReason(Enum):
    TARGET_ACHIEVED = "target_achieved"
    MAX_ITERATIONS = "max_iterations"
    PLATEAU_DETECTED = "plateau_detected"
    CONSECUTIVE_FAILURES = "consecutive_failures"
    USER_INTERRUPTED = "user_interrupted"
    RUNNING = "running"  # Not stopped yet
```

**Class: ImprovementLoop**
```python
class ImprovementLoop:
    def __init__(
        self,
        evals_dir: Path,
        max_iterations: int = 100,
        target_f1: float = 0.90,
        plateau_patience: int = 5,
        min_improvement: float = 0.005,  # 0.5% improvement threshold
        max_consecutive_failures: int = 3
    )

    def should_stop(self) -> tuple[StopReason, str]:
        """Check all stop conditions, return (reason, message) or (RUNNING, "")"""

    def run_single_iteration(self) -> tuple[bool, dict]:
        """Run one iteration: critic -> apply -> extract -> verify -> commit
        Returns (success, metrics)
        Uses existing Phase 5 functions - DO NOT duplicate logic.
        """

    def run(self, resume_from: Optional[int] = None) -> StopReason:
        """Main loop with Rich progress display.
        If resume_from specified, truncate history and continue.
        """
```

**Stop condition logic (in should_stop):**
1. Target achieved: `latest_f1 >= target_f1`
2. Max iterations: `len(history) >= max_iterations`
3. Plateau: `history.get_iterations_since_improvement(min_improvement) >= plateau_patience`
4. Consecutive failures: Track failures, stop after max_consecutive_failures

**run_single_iteration implementation:**
- Import from cli: `load_aggregate_metrics`, `run_extraction`, `run_verification`, `git_commit_iteration`, `get_next_iteration`, `get_eval_ids`
- Import from critic: `load_eval_results`, `aggregate_failure_analysis`, `invoke_critic`, `parse_proposal`
- Import from apply: `apply_proposal`
- Skip user interaction (no present_proposal) - automatically accept proposals
- On critic failure or no proposal: return (False, current_metrics)

**run() with Rich progress:**
```python
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# Show initial state panel
# Progress bar for iterations
# After each iteration: show metrics table (iteration, F1, delta, change)
# On stop: show final summary panel with stop reason
```

**Integration notes:**
- Reuse existing cli.py functions, don't reimplement extraction/verification
- Move `get_eval_ids`, `load_aggregate_metrics`, etc. to module level in cli.py if needed for import
- Console output via Rich (already used in Phase 5)
  </action>
  <verify>
```bash
# Check module imports work
cd /Users/Andrew/Projects/takeoff2
python3 -c "from src.improvement.loop import ImprovementLoop, StopReason; print('Loop module imports OK')"

# Check stop condition logic with mock history
python3 -c "
from pathlib import Path
import sys; sys.path.insert(0, 'src')
from improvement.loop import ImprovementLoop, StopReason
from improvement.history import IterationHistory

# Mock test - just verify class instantiation
loop = ImprovementLoop(Path('evals'), max_iterations=10, target_f1=0.90)
print(f'Loop created: max_iter={loop.max_iterations}, target={loop.target_f1}')
reason, msg = loop.should_stop()
print(f'Initial stop check: {reason}, {msg}')
"
```
  </verify>
  <done>ImprovementLoop class orchestrates automated iterations with stop conditions (target F1, plateau, max iterations), integrates with existing cli.py functions, displays progress via Rich</done>
</task>

<task type="auto">
  <name>Task 3: Update Improvement Module Exports</name>
  <files>src/improvement/__init__.py</files>
  <action>
Update __init__.py to export new loop and history classes.

Add imports:
```python
from .history import IterationHistory, IterationRecord
from .loop import ImprovementLoop, StopReason
```

Update __all__ to include new exports.

Also ensure cli.py helper functions are importable:
- If functions like `get_eval_ids`, `load_aggregate_metrics`, `run_extraction`, `run_verification` are currently defined inside cli.py but needed by loop.py, they should be importable.
- These functions are already at module level in cli.py, so just ensure imports work.
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2
python3 -c "
from src.improvement import (
    ImprovementLoop,
    StopReason,
    IterationHistory,
    IterationRecord,
    InstructionProposal
)
print('All exports available')
"
```
  </verify>
  <done>Improvement module exports IterationHistory, IterationRecord, ImprovementLoop, StopReason alongside existing exports</done>
</task>

</tasks>

<verification>
- [ ] `python3 -c "from src.improvement.history import IterationHistory"` succeeds
- [ ] `python3 -c "from src.improvement.loop import ImprovementLoop, StopReason"` succeeds
- [ ] IterationHistory persists to JSON and loads correctly
- [ ] ImprovementLoop.should_stop() returns correct StopReason for each condition
- [ ] No circular import issues between loop.py and cli.py
</verification>

<success_criteria>
- IterationHistory tracks metrics across iterations with JSON persistence
- ImprovementLoop implements all stop conditions: target F1, max iterations, plateau, consecutive failures
- Loop can be instantiated and stop conditions evaluated
- All new classes exported from improvement module
</success_criteria>

<output>
After completion, create `.planning/phases/06-automated-improvement-loop/06-01-SUMMARY.md`
</output>
