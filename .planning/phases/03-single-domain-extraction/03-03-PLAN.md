---
phase: 03-single-domain-extraction
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/agents/__init__.py
  - src/agents/discovery.py
  - src/agents/extractors/__init__.py
  - src/agents/extractors/base.py
  - src/agents/extractors/project.py
  - src/agents/orchestrator.py
  - src/agents/cli.py
  - pyproject.toml

autonomous: true

user_setup:
  - service: anthropic
    why: "Claude API for vision extraction"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys"

must_haves:
  truths:
    - "User can run extraction on a single eval with one command"
    - "Discovery agent classifies pages and returns DocumentMap"
    - "Project extractor extracts ProjectInfo and EnvelopeInfo from document map"
    - "Orchestrator coordinates discovery -> extraction -> merge flow"
    - "Extraction output is BuildingSpec JSON saved to eval directory"
  artifacts:
    - path: "src/agents/orchestrator.py"
      provides: "LangGraph extraction workflow"
      exports: ["run_extraction"]
      contains: "StateGraph"
    - path: "src/agents/discovery.py"
      provides: "Discovery agent Claude API wrapper"
      exports: ["run_discovery"]
    - path: "src/agents/extractors/project.py"
      provides: "Project extractor Claude API wrapper"
      exports: ["run_project_extractor"]
    - path: "src/agents/cli.py"
      provides: "CLI entry point"
      exports: ["cli"]
  key_links:
    - from: "src/agents/orchestrator.py"
      to: "src/agents/discovery.py"
      via: "import run_discovery"
      pattern: "from agents.discovery import"
    - from: "src/agents/orchestrator.py"
      to: "src/agents/extractors/project.py"
      via: "import run_project_extractor"
      pattern: "from agents.extractors.project import"
    - from: "src/agents/cli.py"
      to: "src/agents/orchestrator.py"
      via: "import run_extraction"
      pattern: "from agents.orchestrator import"
    - from: "pyproject.toml"
      to: "src/agents/cli.py"
      via: "extractor CLI entry point"
      pattern: 'extractor = "agents.cli:cli"'
---

<objective>
Build the extraction orchestrator using LangGraph that coordinates discovery agent and project extractor, plus CLI for running extraction on eval cases.

Purpose: Validates end-to-end extraction pipeline: preprocessed images -> discovery -> extraction -> BuildingSpec JSON.

Output: Working `extractor extract-one` command that produces verified BuildingSpec output.
</objective>

<execution_context>
@/Users/Andrew/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Andrew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-single-domain-extraction/03-RESEARCH.md
@.planning/phases/03-single-domain-extraction/03-01-SUMMARY.md
@.planning/phases/03-single-domain-extraction/03-02-SUMMARY.md

# Schemas to use
@src/schemas/building_spec.py
@src/schemas/discovery.py

# CLI patterns to follow
@src/preprocessor/cli.py
@src/verifier/cli.py

# Agent instructions (for reference, not invoked directly in Python)
@.claude/instructions/discovery/instructions.md
@.claude/instructions/project-extractor/instructions.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create discovery and project extractor Python modules</name>
  <files>src/agents/__init__.py, src/agents/discovery.py, src/agents/extractors/__init__.py, src/agents/extractors/base.py, src/agents/extractors/project.py</files>
  <action>
Create Python modules that wrap Claude API calls for extraction:

1. Create `src/agents/__init__.py`:
   - Export run_discovery, run_extraction

2. Create `src/agents/discovery.py`:
   - Import: anthropic, schemas.discovery (DocumentMap, PageInfo, PageType)
   - Function `run_discovery(page_images: List[Path]) -> DocumentMap`:
     a. Create Anthropic client
     b. Upload page images via Files API (beta.files.upload)
     c. Build content array: [page labels + images] + classification prompt
     d. Use structured outputs with DocumentMap schema
     e. Return parsed DocumentMap
   - Include exponential backoff retry for API errors (429, 500, 503)
   - Log page classifications at INFO level

3. Create `src/agents/extractors/__init__.py`:
   - Export run_project_extractor

4. Create `src/agents/extractors/base.py`:
   - Common utilities: retry decorator, file upload helper
   - Load instruction files from .claude/instructions/ (for prompt construction)

5. Create `src/agents/extractors/project.py`:
   - Import: anthropic, schemas.building_spec (ProjectInfo, EnvelopeInfo)
   - Create combined schema `ProjectExtraction(BaseModel)` with project: ProjectInfo, envelope: EnvelopeInfo
   - Function `run_project_extractor(page_images: List[Path], document_map: DocumentMap) -> ProjectExtraction`:
     a. Filter to relevant pages (schedule_pages + cbecc_pages from document_map)
     b. Upload filtered page images
     c. Build extraction prompt using field guide content
     d. Use structured outputs with ProjectExtraction schema
     e. Return parsed ProjectExtraction
   - Handle case where no relevant pages found (raise ValueError)

Key implementation details from research:
- Use `client.messages.create()` with `response_format` for structured outputs (NOT .parse() which requires beta)
- Model: "claude-sonnet-4-5-20250514" (or similar available model)
- Max tokens: 4096 for extraction responses
- Image-before-text ordering in content array
- PNG images from preprocessed/ directory (already 1568px capped)
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2 && python -c "
from agents.discovery import run_discovery
from agents.extractors.project import run_project_extractor
from schemas.discovery import DocumentMap
from schemas.building_spec import ProjectInfo, EnvelopeInfo
print('Imports successful')
print(f'run_discovery: {run_discovery.__doc__}')
print(f'run_project_extractor: {run_project_extractor.__doc__}')
"
```
  </verify>
  <done>Discovery and project extractor modules import cleanly, functions documented</done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrator with LangGraph workflow</name>
  <files>src/agents/orchestrator.py</files>
  <action>
Create LangGraph-based orchestrator:

Create `src/agents/orchestrator.py`:
1. Define ExtractionState TypedDict:
   - eval_name: str
   - pdf_path: str
   - page_images: List[str]
   - document_map: Optional[DocumentMap]
   - project_extraction: Optional[ProjectExtraction]
   - building_spec: Optional[dict]
   - error: Optional[str]

2. Create node functions:
   - `discovery_node(state) -> dict`: Call run_discovery, return {"document_map": result} or {"error": str}
   - `project_extraction_node(state) -> dict`: Call run_project_extractor, return {"project_extraction": result} or {"error": str}
   - `merge_node(state) -> dict`: Create BuildingSpec from project_extraction, return {"building_spec": spec.model_dump()}

3. Build workflow graph:
   - StateGraph(ExtractionState)
   - Add nodes: discovery, extract_project, merge
   - Edges: discovery -> extract_project -> merge -> END
   - Entry point: discovery

4. Export function `run_extraction(eval_name: str, eval_dir: Path) -> dict`:
   - Find preprocessed images in eval_dir/preprocessed/{pdf_stem}/
   - Find PDF path in eval_dir/
   - Invoke workflow with initial state
   - Return final state (includes building_spec or error)

5. Add logging at each step transition
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2 && python -c "
from agents.orchestrator import run_extraction
print('Orchestrator imports successfully')
print(f'run_extraction signature: {run_extraction.__doc__}')
"
```
  </verify>
  <done>Orchestrator module with LangGraph workflow compiles and exports run_extraction</done>
</task>

<task type="auto">
  <name>Task 3: Create extractor CLI and register entry point</name>
  <files>src/agents/cli.py, pyproject.toml</files>
  <action>
Create CLI following preprocessor/verifier patterns:

1. Create `src/agents/cli.py`:
   - Import click, Path, json
   - Import run_extraction from orchestrator

   Commands:
   a. `extract-one`:
      - Arguments: eval_id (e.g., "chamberlin-circle")
      - Options: --evals-dir (default: evals/), --output (default: extracted.json in eval dir)
      - Action: Run extraction, save BuildingSpec JSON to output path
      - Output: Print success/failure, path to JSON file
      - Error handling: Print error message if extraction fails

   b. `extract-all`:
      - Options: --evals-dir (default: evals/), --skip-existing, --force
      - Action: Run extraction on all evals in manifest.yaml
      - Output: Summary table of results (eval_id, status, output_path)

2. Update `pyproject.toml`:
   - Add langgraph>=0.2.75 to dependencies
   - Add anthropic>=0.45 to dependencies
   - Add CLI entry point: extractor = "agents.cli:cli"
   - Add src/agents to hatch wheel packages

3. Test CLI registration:
   - pip install -e .
   - extractor --help
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2 && pip install -e . && extractor --help
extractor extract-one --help
```
  </verify>
  <done>extractor CLI registered, extract-one and extract-all commands show in help</done>
</task>

</tasks>

<verification>
End-to-end verification (requires ANTHROPIC_API_KEY):

1. Run extraction on one eval:
```bash
cd /Users/Andrew/Projects/takeoff2
export ANTHROPIC_API_KEY=your_key_here  # User must set this
extractor extract-one chamberlin-circle
```

2. Verify output exists:
```bash
ls -la /Users/Andrew/Projects/takeoff2/evals/chamberlin-circle/extracted.json
cat /Users/Andrew/Projects/takeoff2/evals/chamberlin-circle/extracted.json | python -m json.tool | head -30
```

3. Run verifier on extraction:
```bash
verifier verify-one chamberlin-circle --extracted evals/chamberlin-circle/extracted.json
```

Note: Full verification requires ANTHROPIC_API_KEY. If not available, verify:
- CLI commands register correctly
- Modules import without errors
- Type annotations match schemas
</verification>

<success_criteria>
- extractor CLI registered with extract-one and extract-all commands
- run_extraction orchestrates discovery -> extraction -> merge flow
- BuildingSpec JSON saved to eval directory
- Extraction can be verified against ground truth using existing verifier
- Baseline F1 measurable for project/envelope domain
</success_criteria>

<output>
After completion, create `.planning/phases/03-single-domain-extraction/03-03-SUMMARY.md`
</output>
