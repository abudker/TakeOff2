---
phase: 05-manual-improvement-loop
plan: 04
type: execute
wave: 4
depends_on: ["05-03"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "User can run improve command end-to-end with Claude CLI"
    - "Critic generates valid proposal from real eval results"
    - "Proposal review workflow functions correctly"
    - "At least one iteration demonstrates the full loop"
  artifacts: []
  key_links:
    - from: "python -m improvement improve"
      to: "claude --agent critic"
      via: "subprocess invocation"
      pattern: "critic"
---

<objective>
Verify the complete improvement loop works end-to-end with real Claude CLI invocation.

Purpose: Confirm that all components integrate correctly: failure analysis, critic invocation, proposal review, application, re-extraction, re-verification, and metrics comparison. This validates Phase 5 requirements are met.

Output: Verified working improvement loop with at least one demonstration iteration.
</objective>

<context>
@.planning/PROJECT.md
@.planning/phases/05-manual-improvement-loop/05-CONTEXT.md

# All Phase 5 modules
@src/improvement/critic.py
@src/improvement/review.py
@src/improvement/apply.py
@src/improvement/cli.py
@.claude/agents/critic.md
@.claude/instructions/critic/instructions.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run Improvement Loop Dry Run</name>
  <files></files>
  <action>
Run the improvement loop with --skip-extraction to verify all Python components work before full extraction.

```bash
cd /Users/Andrew/Projects/takeoff2

# First ensure we have eval results to analyze
python -m verifier verify-all --evals-dir evals --save

# Run improvement with skip-extraction to test critic invocation
python -m improvement improve --evals-dir evals --skip-extraction --no-commit
```

This will:
1. Load eval results and aggregate failure patterns
2. Invoke critic agent via Claude CLI
3. Present proposal for user review
4. If accepted, apply change to instruction file
5. Skip re-extraction (use existing)
6. Re-run verification
7. Show metrics comparison
8. Skip commit (--no-commit)

**Expected flow:**
- Analysis loads successfully
- Critic invocation produces proposal JSON
- Rich-formatted proposal displays
- User can accept/edit/reject
- If accepted, instruction file modified with version bump

**If critic fails:** Check that .claude/agents/critic.md exists and Claude CLI is available (`which claude`).
  </action>
  <verify>
The dry run completes without Python errors. Critic produces parseable proposal. User can interact with proposal review. If accepted, instruction file is modified.
  </verify>
  <done>
Dry run of improvement loop works: failure analysis, critic invocation, proposal review, and instruction modification all function correctly.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify Full Improvement Loop</name>
  <what-built>
Complete improvement loop with:
- Critic agent that analyzes verification failures
- Interactive proposal review with Rich UI
- Proposal application with version bumping
- Integration with extract-all and verify-all CLIs
- Metrics comparison display
- Git auto-commit with metrics
  </what-built>
  <how-to-verify>
1. Run the improvement loop:
   ```bash
   cd /Users/Andrew/Projects/takeoff2
   python -m improvement improve --evals-dir evals
   ```

2. When proposal displays, verify:
   - Failure pattern makes sense for the data
   - Hypothesis is reasonable
   - Proposed change is concrete markdown (not vague)
   - Expected impact is stated

3. Choose "Accept" (a) to apply the proposal

4. Watch the loop:
   - Extraction runs on all 5 evals
   - Verification runs on all 5 evals
   - Metrics comparison table displays

5. Check the results:
   - Instruction file was modified (cat the target file)
   - Version was bumped in header
   - Git commit was created with metrics:
     ```bash
     git log -1 --format="%B"
     ```

6. Verify iteration tracking:
   - Check for instruction snapshots:
     ```bash
     ls evals/chamberlin-circle/results/iteration-*/instruction-changes/
     ```

7. Test rollback (optional):
   ```bash
   python -m improvement rollback 1 --evals-dir evals
   cat .claude/instructions/<agent>/instructions.md  # Should be restored
   ```
  </how-to-verify>
  <resume-signal>
Type "approved" if the full loop works correctly.
Or describe any issues encountered.
  </resume-signal>
</task>

</tasks>

<verification>
Phase 5 success criteria from ROADMAP.md:
1. [x] Critic agent analyzes verification results and identifies specific failure patterns
2. [x] Critic proposes targeted changes to instruction files (not agent definitions)
3. [x] Proposals include clear rationale and expected impact
4. [x] User can manually apply a proposal, re-run extraction/eval, and measure improvement
5. [ ] At least one iteration demonstrates measurable F1 improvement (verified in checkpoint)
</verification>

<success_criteria>
- Full improvement loop runs without errors
- Critic generates valid, actionable proposal
- Proposal is applied to instruction file with version bump
- Re-extraction and re-verification complete
- Before/after metrics comparison shows
- Git commit includes metrics delta
- Iteration snapshots saved for rollback
</success_criteria>

<output>
After completion, create `.planning/phases/05-manual-improvement-loop/05-04-SUMMARY.md`
</output>
