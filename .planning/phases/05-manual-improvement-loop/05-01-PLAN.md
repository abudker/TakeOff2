---
phase: 05-manual-improvement-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .claude/agents/critic.md
  - .claude/instructions/critic/instructions.md
  - .claude/instructions/critic/proposal-format.md
  - src/improvement/__init__.py
  - src/improvement/critic.py
autonomous: true

must_haves:
  truths:
    - "Critic agent definition exists and follows thin-agent pattern"
    - "Failure analysis aggregates discrepancies across all evals by error type and domain"
    - "Critic can be invoked via subprocess and returns parseable proposal JSON"
  artifacts:
    - path: ".claude/agents/critic.md"
      provides: "Critic agent definition"
      contains: "instructions:"
    - path: ".claude/instructions/critic/instructions.md"
      provides: "Critic behavior and workflow"
      min_lines: 50
    - path: ".claude/instructions/critic/proposal-format.md"
      provides: "Proposal JSON schema and examples"
      contains: "target_file"
    - path: "src/improvement/critic.py"
      provides: "Failure analysis and critic invocation"
      exports: ["aggregate_failure_analysis", "invoke_critic", "parse_proposal"]
  key_links:
    - from: "src/improvement/critic.py"
      to: "evals/*/results/*/eval-results.json"
      via: "json.load in aggregate_failure_analysis"
      pattern: "eval-results\\.json"
    - from: "src/improvement/critic.py"
      to: "claude subprocess"
      via: "subprocess.run with --agent critic"
      pattern: "subprocess\\.run.*claude"
---

<objective>
Create the critic agent and failure analysis infrastructure for the improvement loop.

Purpose: The critic agent analyzes verification results (implementation-blind) and proposes targeted changes to instruction files. This is the foundation of the self-improvement system.

Output: Critic agent definition, instruction files, and Python module for failure analysis and critic invocation.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-manual-improvement-loop/05-CONTEXT.md
@.planning/phases/05-manual-improvement-loop/05-RESEARCH.md

# Prior phase context
@.claude/agents/project-extractor.md
@.claude/instructions/project-extractor/instructions.md
@src/agents/orchestrator.py
@evals/chamberlin-circle/results/iteration-005/eval-results.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Critic Agent Definition and Instructions</name>
  <files>
    .claude/agents/critic.md
    .claude/instructions/critic/instructions.md
    .claude/instructions/critic/proposal-format.md
  </files>
  <action>
Create critic agent following established thin-agent pattern (under 50 lines definition, behavior in instructions).

**critic.md agent definition:**
- Name: critic
- Description: Analyzes extraction failures and proposes instruction file improvements
- Tools: Read (to examine instruction files if needed)
- Instructions reference: .claude/instructions/critic/instructions.md

**instructions.md must include:**
1. Implementation-blind principle: Analyze ONLY verification results, NOT agent code
2. Failure pattern recognition:
   - Group errors by type (omission, hallucination, wrong_value, format_error)
   - Group errors by domain (project, envelope, walls, windows, zones, hvac, dhw)
   - Identify dominant patterns
3. Hypothesis generation:
   - For each pattern, hypothesize WHY it's happening
   - Focus on what the INSTRUCTION is missing, not code
4. Proposal generation:
   - Target ONE instruction file per proposal (Phase 5 = one proposal at a time)
   - Include concrete markdown text to add/modify
   - Version bump rules (minor for add_section/modify_section, patch for clarify_rule)
5. Constraint: May ONLY propose changes to files in .claude/instructions/

**proposal-format.md must define:**
JSON schema for proposals with:
- target_file: Path to instruction file
- current_version: Extracted from file header (e.g., "v1.0.0")
- proposed_version: Bumped version
- change_type: "add_section" | "modify_section" | "clarify_rule"
- failure_pattern: What went wrong (from analysis)
- hypothesis: Why it went wrong
- proposed_change: Exact markdown text to add
- expected_impact: What should improve
- affected_error_types: List of error types this targets
- affected_domains: List of domains this targets

Include 2-3 example proposals showing good format.
  </action>
  <verify>
Files exist at expected paths:
- ls .claude/agents/critic.md
- ls .claude/instructions/critic/instructions.md
- ls .claude/instructions/critic/proposal-format.md

Agent definition is under 50 lines:
- wc -l .claude/agents/critic.md
  </verify>
  <done>
Critic agent definition exists with instructions reference. Instructions explain implementation-blind analysis, failure pattern recognition, and proposal generation. Proposal format is documented with JSON schema and examples.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Failure Analysis Module</name>
  <files>
    src/improvement/__init__.py
    src/improvement/critic.py
  </files>
  <action>
Create src/improvement/ package with critic.py module.

**critic.py must implement:**

```python
from pathlib import Path
from typing import Dict, List, Any, Optional
import json

def find_latest_iteration(eval_dir: Path) -> int:
    """Find the latest iteration number in eval's results directory."""
    # Look for iteration-NNN folders, return highest N

def load_eval_results(evals_dir: Path, eval_ids: List[str]) -> List[Dict[str, Any]]:
    """Load eval-results.json from latest iteration of each eval."""
    # For each eval_id, find latest iteration, load eval-results.json
    # Return list of result dicts

def aggregate_failure_analysis(eval_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Aggregate failure patterns across all evals.

    Implementation-blind: only looks at discrepancies and metrics.

    Returns dict with:
    - num_evals: int
    - total_discrepancies: int
    - aggregate_f1: float (macro average)
    - errors_by_type: {omission: N, hallucination: N, ...}
    - errors_by_domain: {project: N, walls: N, ...}
    - dominant_error_type: str
    - dominant_domain: str
    - sample_discrepancies: List[Dict] (first 20 for context)
    """
    # Implementation per RESEARCH.md Pattern 1

def format_analysis_for_critic(analysis: Dict[str, Any]) -> str:
    """Format analysis as text prompt for critic agent."""
    # Create structured text that critic can process
    # Include summary stats and sample discrepancies
```

Domain extraction from field_path:
- "project.run_id" -> "project"
- "walls[0].name" -> "walls"
- "envelope.conditioned_floor_area" -> "envelope"
- Use regex: `field_path.split(".")[0].split("[")[0]`
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2 && python -c "
from improvement.critic import (
    find_latest_iteration,
    load_eval_results,
    aggregate_failure_analysis,
    format_analysis_for_critic
)
from pathlib import Path

# Test with existing eval data
evals_dir = Path('evals')
results = load_eval_results(evals_dir, ['chamberlin-circle'])
assert len(results) == 1
assert 'metrics' in results[0]
assert 'discrepancies' in results[0]

analysis = aggregate_failure_analysis(results)
assert 'errors_by_type' in analysis
assert 'errors_by_domain' in analysis
assert 'dominant_error_type' in analysis
print('Failure analysis module OK')
"
```
  </verify>
  <done>
src/improvement/critic.py exists with functions to load eval results, aggregate failure patterns by error type and domain, and format analysis for critic agent.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Critic Invocation Module</name>
  <files>src/improvement/critic.py</files>
  <action>
Extend critic.py with critic agent invocation and proposal parsing.

**Add to critic.py:**

```python
import subprocess
import re
from dataclasses import dataclass

@dataclass
class InstructionProposal:
    """A proposed change to an instruction file."""
    target_file: str
    current_version: str
    proposed_version: str
    change_type: str  # add_section | modify_section | clarify_rule
    failure_pattern: str
    hypothesis: str
    proposed_change: str
    expected_impact: str
    affected_error_types: List[str]
    affected_domains: List[str]
    estimated_f1_delta: Optional[float] = None

def invoke_critic(
    analysis: Dict[str, Any],
    instructions_dir: Path,
    project_root: Path
) -> str:
    """
    Invoke critic agent via Claude CLI subprocess.

    Args:
        analysis: Aggregated failure analysis
        instructions_dir: Path to .claude/instructions/
        project_root: Project root for working directory

    Returns:
        Raw output from critic agent
    """
    # Format analysis as prompt text
    prompt = format_analysis_for_critic(analysis)

    # List available instruction files for critic context
    instruction_files = list(instructions_dir.rglob("*.md"))
    files_list = "\n".join(f"- {f.relative_to(project_root)}" for f in instruction_files)

    full_prompt = f"""Analyze the following extraction failure patterns and propose ONE instruction file improvement.

## Failure Analysis

{prompt}

## Available Instruction Files

{files_list}

## Your Task

Based on the failure patterns above, generate a proposal to improve ONE instruction file.
Output your proposal as JSON following the schema in .claude/instructions/critic/proposal-format.md
"""

    # Invoke via subprocess
    result = subprocess.run(
        ["claude", "--agent", "critic", "--print", full_prompt],
        cwd=project_root,
        capture_output=True,
        text=True,
        timeout=300  # 5 min timeout
    )

    if result.returncode != 0:
        raise RuntimeError(f"Critic agent failed: {result.stderr}")

    return result.stdout

def parse_proposal(critic_output: str) -> Optional[InstructionProposal]:
    """
    Parse critic output to extract proposal JSON.

    Handles cases where JSON is embedded in markdown code blocks.
    Returns None if no valid proposal found.
    """
    # Try to find JSON in output (may be in code block)
    # Pattern: ```json ... ``` or raw JSON object
    json_match = re.search(r'```json\s*(.*?)\s*```', critic_output, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        # Try to find raw JSON object
        json_match = re.search(r'\{[^{}]*"target_file"[^{}]*\}', critic_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            return None

    try:
        data = json.loads(json_str)
        return InstructionProposal(
            target_file=data["target_file"],
            current_version=data.get("current_version", "v1.0.0"),
            proposed_version=data.get("proposed_version", "v1.0.1"),
            change_type=data.get("change_type", "add_section"),
            failure_pattern=data["failure_pattern"],
            hypothesis=data["hypothesis"],
            proposed_change=data["proposed_change"],
            expected_impact=data["expected_impact"],
            affected_error_types=data.get("affected_error_types", []),
            affected_domains=data.get("affected_domains", []),
            estimated_f1_delta=data.get("estimated_f1_delta")
        )
    except (json.JSONDecodeError, KeyError) as e:
        return None
```

Note: Use `claude --agent critic --print` to invoke the critic agent without interactive mode.
  </action>
  <verify>
```bash
cd /Users/Andrew/Projects/takeoff2 && python -c "
from improvement.critic import InstructionProposal, parse_proposal

# Test proposal parsing with mock output
mock_output = '''
Here is my analysis...

\`\`\`json
{
  \"target_file\": \".claude/instructions/project-extractor/instructions.md\",
  \"current_version\": \"v1.0.0\",
  \"proposed_version\": \"v1.1.0\",
  \"change_type\": \"add_section\",
  \"failure_pattern\": \"High omission rate in project fields\",
  \"hypothesis\": \"Extractor not checking all required fields\",
  \"proposed_change\": \"## Required Fields Checklist...\",
  \"expected_impact\": \"Reduce omissions by 50%\",
  \"affected_error_types\": [\"omission\"],
  \"affected_domains\": [\"project\"]
}
\`\`\`
'''

proposal = parse_proposal(mock_output)
assert proposal is not None
assert proposal.target_file == '.claude/instructions/project-extractor/instructions.md'
assert proposal.change_type == 'add_section'
print('Critic invocation module OK')
"
```
  </verify>
  <done>
critic.py includes invoke_critic() to call critic agent via subprocess and parse_proposal() to extract structured InstructionProposal from output. Proposal parsing handles JSON in code blocks.
  </done>
</task>

</tasks>

<verification>
All tasks complete when:
1. Critic agent files exist in .claude/agents/ and .claude/instructions/critic/
2. src/improvement/critic.py exports required functions
3. Failure analysis correctly aggregates errors by type and domain
4. Proposal parsing handles JSON in various formats
</verification>

<success_criteria>
- Critic agent definition under 50 lines, behavior in separate instruction files
- Failure analysis loads eval-results.json and aggregates by error type/domain
- invoke_critic() calls Claude CLI with --agent critic flag
- parse_proposal() extracts InstructionProposal from critic output
</success_criteria>

<output>
After completion, create `.planning/phases/05-manual-improvement-loop/05-01-SUMMARY.md`
</output>
