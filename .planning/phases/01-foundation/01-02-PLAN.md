---
phase: 01-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/verifier/__init__.py
  - src/verifier/cli.py
  - src/verifier/compare.py
  - src/verifier/metrics.py
  - src/verifier/categorize.py
  - src/schemas/building_spec.py
  - src/schemas/field_mapping.yaml
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "User can run 'python -m verifier verify-one <eval_id> <extracted.json>' and see P/R/F1"
    - "User can run 'python -m verifier verify-all' and see aggregate metrics across all 5 evals"
    - "Verifier outputs field-level discrepancies with error type categorization"
    - "Verifier handles CSV ground truth and JSON extraction comparison correctly"
  artifacts:
    - path: "src/verifier/cli.py"
      provides: "CLI entry point with verify-one and verify-all commands"
      exports: ["cli", "verify_one", "verify_all"]
    - path: "src/verifier/compare.py"
      provides: "Field-level comparison logic"
      exports: ["compare_fields", "FieldDiscrepancy"]
    - path: "src/verifier/metrics.py"
      provides: "Precision/recall/F1 computation"
      exports: ["compute_field_level_metrics"]
    - path: "src/verifier/categorize.py"
      provides: "Error type categorization"
      exports: ["categorize_error"]
    - path: "src/schemas/building_spec.py"
      provides: "Pydantic schema for extraction output"
      contains: "class BuildingSpec"
    - path: "src/schemas/field_mapping.yaml"
      provides: "CSV column to JSON field path mapping"
      contains: "csv_to_json"
    - path: "pyproject.toml"
      provides: "Project dependencies and CLI entry point"
      contains: ["pydantic", "pandas", "click"]
  key_links:
    - from: "src/verifier/cli.py"
      to: "src/verifier/compare.py"
      via: "import compare_fields"
      pattern: "from.*compare import"
    - from: "src/verifier/cli.py"
      to: "src/verifier/metrics.py"
      via: "import compute_field_level_metrics"
      pattern: "from.*metrics import"
    - from: "src/verifier/compare.py"
      to: "src/schemas/field_mapping.yaml"
      via: "loads field mapping"
      pattern: "field_mapping"
---

<objective>
Build the core verifier infrastructure: CLI interface, field-level comparison logic, metric computation, and error categorization.

Purpose: Enable measuring extraction quality with precision/recall/F1 metrics. This is the foundation for all improvement iterations - you cannot improve what you cannot measure.

Output: Working verifier CLI that can run on single evals or all 5 evals and output metrics + discrepancies.
</objective>

<execution_context>
@/Users/Andrew/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Andrew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@evals/manifest.yaml
@evals/lamb-adu/ground_truth.csv

Research recommends: Pydantic for schema validation, pandas for CSV parsing, click for CLI. Field-level P/R/F1 computation (not document-level). CSV columns map to nested JSON paths via explicit mapping file.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure and dependencies</name>
  <files>
pyproject.toml
src/verifier/__init__.py
src/schemas/__init__.py
  </files>
  <action>
Create the Python project structure with dependencies.

**pyproject.toml**:
```toml
[project]
name = "takeoff-verifier"
version = "0.1.0"
description = "Evaluation infrastructure for Takeoff v2 extraction system"
requires-python = ">=3.11"
dependencies = [
    "pydantic>=2.10",
    "pandas>=2.2",
    "click>=8.1",
    "jinja2>=3.1",
    "pyyaml>=6.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3",
    "pytest-html>=4.1",
]

[project.scripts]
verifier = "verifier.cli:cli"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

Create directory structure:
```bash
mkdir -p src/verifier
mkdir -p src/schemas
touch src/__init__.py
touch src/verifier/__init__.py
touch src/schemas/__init__.py
```

**src/verifier/__init__.py**:
```python
"""Takeoff v2 Verifier - Evaluate extraction quality against ground truth."""
from .compare import compare_fields, FieldDiscrepancy
from .metrics import compute_field_level_metrics
from .categorize import categorize_error

__all__ = [
    "compare_fields",
    "FieldDiscrepancy",
    "compute_field_level_metrics",
    "categorize_error",
]
```

After creating files, install the package in editable mode:
```bash
pip install -e ".[dev]"
```
  </action>
  <verify>
- pyproject.toml exists with all dependencies
- src/verifier/__init__.py exports the main functions
- `pip install -e ".[dev]"` succeeds
- `python -c "import verifier"` works
  </verify>
  <done>Project structure created and dependencies installed</done>
</task>

<task type="auto">
  <name>Task 2: Create Pydantic schema and field mapping</name>
  <files>
src/schemas/building_spec.py
src/schemas/field_mapping.yaml
  </files>
  <action>
Create the Pydantic schema for extraction output and CSV-to-JSON field mapping.

**src/schemas/building_spec.py**:
Define Pydantic models matching the ground truth CSV structure. Based on lamb-adu ground_truth.csv:

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from enum import Enum

class FuelType(str, Enum):
    ALL_ELECTRIC = "All Electric"
    NATURAL_GAS = "Natural Gas"
    MIXED = "Mixed"

class HouseType(str, Enum):
    SINGLE_FAMILY = "Single Family"
    MULTI_FAMILY = "Multi Family"

class ProjectInfo(BaseModel):
    """Project metadata."""
    run_title: str = Field(description="Project title")
    address: str = Field(description="Street address")
    city: str = Field(description="City name")
    climate_zone: int = Field(ge=1, le=16, description="CA climate zone 1-16")
    fuel_type: FuelType = Field(description="Fuel type")
    house_type: HouseType = Field(description="House type")
    dwelling_units: int = Field(ge=1, description="Number of dwelling units")
    stories: int = Field(ge=1, description="Number of stories")
    bedrooms: int = Field(ge=0, description="Number of bedrooms")

class EnvelopeInfo(BaseModel):
    """Building envelope data."""
    conditioned_floor_area: float = Field(gt=0, description="CFA in sq ft")
    window_area: float = Field(ge=0, description="Total window area sq ft")
    window_to_floor_ratio: float = Field(ge=0, le=1, description="WWR")
    exterior_wall_area: float = Field(ge=0, description="Exterior wall area sq ft")
    fenestration_u_factor: Optional[float] = Field(default=None, description="Area-weighted U-factor")

class ZoneInfo(BaseModel):
    """Zone data."""
    name: str
    floor_area: float = Field(ge=0)
    volume: float = Field(ge=0)

class WallInfo(BaseModel):
    """Wall data."""
    name: str
    orientation: str
    area: float = Field(ge=0)
    r_value: Optional[float] = Field(default=None, ge=0)

class WindowInfo(BaseModel):
    """Window/fenestration data."""
    name: str
    area: float = Field(ge=0)
    u_factor: Optional[float] = Field(default=None)
    shgc: Optional[float] = Field(default=None)

class HVACSystem(BaseModel):
    """HVAC system data."""
    name: str
    system_type: str
    heating_capacity: Optional[float] = None
    cooling_capacity: Optional[float] = None
    efficiency_heating: Optional[float] = None  # HSPF, AFUE
    efficiency_cooling: Optional[float] = None  # SEER

class WaterHeater(BaseModel):
    """Water heater data."""
    name: str
    system_type: str
    capacity: Optional[float] = None
    efficiency: Optional[float] = None  # EF or UEF

class BuildingSpec(BaseModel):
    """Complete building specification from extraction."""
    project: ProjectInfo
    envelope: EnvelopeInfo
    zones: List[ZoneInfo] = Field(default_factory=list)
    walls: List[WallInfo] = Field(default_factory=list)
    windows: List[WindowInfo] = Field(default_factory=list)
    hvac_systems: List[HVACSystem] = Field(default_factory=list)
    water_heaters: List[WaterHeater] = Field(default_factory=list)
```

**src/schemas/field_mapping.yaml**:
Map CSV column names (from ground_truth.csv) to JSON field paths:

```yaml
# Field mapping from ground truth CSV columns to JSON field paths
# CSV format: "Column Name" -> "json.field.path"

csv_to_json:
  # Project fields
  "Run Title": "project.run_title"
  "Address": "project.address"
  "City": "project.city"
  "Climate Zone": "project.climate_zone"
  "Fuel Type": "project.fuel_type"
  "House Type": "project.house_type"
  "Dwelling Units": "project.dwelling_units"
  "Stories": "project.stories"
  "Bedrooms": "project.bedrooms"

  # Envelope fields
  "Conditioned Floor Area": "envelope.conditioned_floor_area"
  "Total Conditioned Zone Window Area": "envelope.window_area"
  "Window to Floor Area Ratio": "envelope.window_to_floor_ratio"
  "Exterior Wall Area": "envelope.exterior_wall_area"
  "Area-Weighted Fenestration Ufactor": "envelope.fenestration_u_factor"

# Field types for comparison logic
field_types:
  numeric:
    - "project.climate_zone"
    - "project.dwelling_units"
    - "project.stories"
    - "project.bedrooms"
    - "envelope.conditioned_floor_area"
    - "envelope.window_area"
    - "envelope.window_to_floor_ratio"
    - "envelope.exterior_wall_area"
    - "envelope.fenestration_u_factor"
  string:
    - "project.run_title"
    - "project.address"
    - "project.city"
    - "project.fuel_type"
    - "project.house_type"

# Tolerance settings for numeric comparisons
tolerances:
  default:
    percent: 0.5  # ±0.5%
    absolute: 0.01  # ±0.01 units
  area:
    percent: 1.0  # ±1% for areas
    absolute: 1.0  # ±1 sq ft
  ratio:
    percent: 0.5
    absolute: 0.001
```
  </action>
  <verify>
- src/schemas/building_spec.py defines BuildingSpec with all nested models
- src/schemas/field_mapping.yaml has csv_to_json mapping for main fields
- `python -c "from schemas.building_spec import BuildingSpec"` works
  </verify>
  <done>Pydantic schema and field mapping created</done>
</task>

<task type="auto">
  <name>Task 3: Implement comparison, metrics, and categorization</name>
  <files>
src/verifier/compare.py
src/verifier/metrics.py
src/verifier/categorize.py
  </files>
  <action>
Implement the core verifier logic.

**src/verifier/compare.py**:
```python
"""Field-level comparison logic for extraction evaluation."""
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from pathlib import Path
import yaml

@dataclass
class FieldDiscrepancy:
    """Represents a single field-level discrepancy."""
    field_path: str
    expected: Any
    actual: Any
    error_type: str  # omission, hallucination, format_error, wrong_value

def load_field_mapping() -> Dict:
    """Load field mapping from YAML config."""
    mapping_path = Path(__file__).parent.parent / "schemas" / "field_mapping.yaml"
    with open(mapping_path) as f:
        return yaml.safe_load(f)

def get_nested_value(data: Dict, path: str) -> Any:
    """Navigate nested dict using dot-separated path."""
    keys = path.split(".")
    value = data
    for key in keys:
        if isinstance(value, dict) and key in value:
            value = value[key]
        else:
            return None
    return value

def set_nested_value(data: Dict, path: str, value: Any):
    """Set value in nested dict using dot-separated path."""
    keys = path.split(".")
    for key in keys[:-1]:
        data = data.setdefault(key, {})
    data[keys[-1]] = value

def flatten_dict(data: Dict, prefix: str = "") -> Dict[str, Any]:
    """Flatten nested dict to {path: value} pairs."""
    result = {}
    for key, value in data.items():
        path = f"{prefix}.{key}" if prefix else key
        if isinstance(value, dict):
            result.update(flatten_dict(value, path))
        elif isinstance(value, list):
            # For lists, use index in path
            for i, item in enumerate(value):
                if isinstance(item, dict):
                    result.update(flatten_dict(item, f"{path}[{i}]"))
                else:
                    result[f"{path}[{i}]"] = item
        else:
            result[path] = value
    return result

def values_match(expected: Any, actual: Any, field_path: str, tolerances: Dict) -> bool:
    """Compare two values with appropriate logic based on type."""
    if expected is None and actual is None:
        return True
    if expected is None or actual is None:
        return False

    # Numeric comparison with tolerance
    if isinstance(expected, (int, float)) and isinstance(actual, (int, float)):
        # Determine tolerance based on field type
        if "area" in field_path.lower():
            tol = tolerances.get("area", tolerances["default"])
        elif "ratio" in field_path.lower():
            tol = tolerances.get("ratio", tolerances["default"])
        else:
            tol = tolerances["default"]

        abs_diff = abs(expected - actual)
        rel_diff = abs_diff / abs(expected) if expected != 0 else abs_diff

        return (rel_diff <= tol["percent"] / 100 or
                abs_diff <= tol["absolute"])

    # String comparison (case-insensitive, trimmed)
    if isinstance(expected, str) and isinstance(actual, str):
        return expected.strip().lower() == actual.strip().lower()

    # Boolean exact match
    if isinstance(expected, bool) and isinstance(actual, bool):
        return expected == actual

    # Type mismatch - try coercion
    try:
        if isinstance(expected, (int, float)):
            return values_match(expected, float(actual), field_path, tolerances)
        if isinstance(expected, str):
            return values_match(expected, str(actual), field_path, tolerances)
    except (ValueError, TypeError):
        pass

    return expected == actual

def compare_fields(
    ground_truth: Dict[str, Any],
    extracted: Dict[str, Any],
    mapping: Optional[Dict] = None
) -> List[FieldDiscrepancy]:
    """
    Compare extracted data against ground truth at field level.

    Args:
        ground_truth: Dict with ground truth values (flattened from CSV)
        extracted: Dict with extracted values (from JSON)
        mapping: Optional field mapping config

    Returns:
        List of FieldDiscrepancy objects for each mismatch
    """
    if mapping is None:
        mapping = load_field_mapping()

    tolerances = mapping.get("tolerances", {"default": {"percent": 0.5, "absolute": 0.01}})
    discrepancies = []

    gt_flat = flatten_dict(ground_truth)
    ext_flat = flatten_dict(extracted)

    # Check each ground truth field
    for gt_path, expected_value in gt_flat.items():
        actual_value = ext_flat.get(gt_path)

        if actual_value is None:
            discrepancies.append(FieldDiscrepancy(
                field_path=gt_path,
                expected=expected_value,
                actual=None,
                error_type="omission"
            ))
        elif not values_match(expected_value, actual_value, gt_path, tolerances):
            # Determine if it's a format error or wrong value
            if type(expected_value) != type(actual_value):
                error_type = "format_error"
            else:
                error_type = "wrong_value"
            discrepancies.append(FieldDiscrepancy(
                field_path=gt_path,
                expected=expected_value,
                actual=actual_value,
                error_type=error_type
            ))

    # Check for hallucinated fields (in extracted but not in ground truth)
    for ext_path, actual_value in ext_flat.items():
        if ext_path not in gt_flat:
            discrepancies.append(FieldDiscrepancy(
                field_path=ext_path,
                expected=None,
                actual=actual_value,
                error_type="hallucination"
            ))

    return discrepancies
```

**src/verifier/metrics.py**:
```python
"""Precision/recall/F1 computation at field level."""
from typing import Dict, List
from .compare import FieldDiscrepancy

def compute_field_level_metrics(
    discrepancies: List[FieldDiscrepancy],
    total_fields_gt: int,
    total_fields_extracted: int
) -> Dict[str, float]:
    """
    Compute precision, recall, F1 at field level.

    Each field is a binary classification:
    - True Positive (TP): field in ground truth that was correctly extracted
    - False Positive (FP): field in extracted that is wrong or hallucinated
    - False Negative (FN): field in ground truth that was omitted

    Args:
        discrepancies: List of field discrepancies
        total_fields_gt: Total fields in ground truth
        total_fields_extracted: Total fields in extracted JSON

    Returns:
        Dict with precision, recall, f1, and breakdown
    """
    # Count error types
    omissions = sum(1 for d in discrepancies if d.error_type == "omission")
    hallucinations = sum(1 for d in discrepancies if d.error_type == "hallucination")
    wrong_values = sum(1 for d in discrepancies if d.error_type == "wrong_value")
    format_errors = sum(1 for d in discrepancies if d.error_type == "format_error")

    # Compute TP/FP/FN
    # TP = fields in GT that match (correct)
    # FN = omissions (in GT but not extracted)
    # FP = hallucinations + wrong_values + format_errors (in extracted but wrong)

    true_positives = total_fields_gt - omissions - wrong_values - format_errors
    false_positives = hallucinations + wrong_values + format_errors
    false_negatives = omissions

    # Compute metrics with zero-division handling
    precision = (true_positives / (true_positives + false_positives)
                 if (true_positives + false_positives) > 0 else 0.0)
    recall = (true_positives / (true_positives + false_negatives)
              if (true_positives + false_negatives) > 0 else 0.0)
    f1 = (2 * (precision * recall) / (precision + recall)
          if (precision + recall) > 0 else 0.0)

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "total_fields_gt": total_fields_gt,
        "total_fields_extracted": total_fields_extracted,
        "correct_fields": true_positives,
        "errors_by_type": {
            "omission": omissions,
            "hallucination": hallucinations,
            "wrong_value": wrong_values,
            "format_error": format_errors,
        }
    }

def compute_aggregate_metrics(eval_metrics: List[Dict[str, float]]) -> Dict[str, float]:
    """
    Compute aggregate metrics across multiple evals (macro-averaging).

    Args:
        eval_metrics: List of per-eval metric dicts

    Returns:
        Dict with aggregated metrics
    """
    if not eval_metrics:
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}

    n = len(eval_metrics)
    return {
        "precision": sum(m["precision"] for m in eval_metrics) / n,
        "recall": sum(m["recall"] for m in eval_metrics) / n,
        "f1": sum(m["f1"] for m in eval_metrics) / n,
        "total_evals": n,
        "per_eval": eval_metrics,
    }
```

**src/verifier/categorize.py**:
```python
"""Error type categorization for extraction discrepancies."""
from typing import Any
from .compare import FieldDiscrepancy

def categorize_error(expected: Any, actual: Any) -> str:
    """
    Categorize the error type for a field discrepancy.

    Error types:
    - omission: Expected field missing from extraction
    - hallucination: Field present in extraction but not in ground truth
    - format_error: Field present but wrong type/format
    - wrong_value: Field present, correct type, but incorrect value
    """
    if expected is not None and actual is None:
        return "omission"

    if expected is None and actual is not None:
        return "hallucination"

    # Check for type mismatch (format_error)
    expected_type = type(expected)
    actual_type = type(actual)

    # Consider int/float as compatible numeric types
    numeric_types = (int, float)
    if isinstance(expected, numeric_types) and isinstance(actual, numeric_types):
        return "wrong_value"

    if expected_type != actual_type:
        return "format_error"

    return "wrong_value"

def get_improvement_hint(error_type: str, field_path: str) -> str:
    """
    Get a hint for how to improve extraction based on error type.

    Used by critic agent to generate improvement proposals.
    """
    hints = {
        "omission": f"Extractor missed '{field_path}'. Add to extraction checklist or emphasize in instructions.",
        "hallucination": f"Extractor fabricated '{field_path}'. Add validation to require evidence in PDF.",
        "format_error": f"Extractor got wrong type for '{field_path}'. Add type hints to extraction schema.",
        "wrong_value": f"Extractor got wrong value for '{field_path}'. May need domain-specific guidance.",
    }
    return hints.get(error_type, "Unknown error type")
```
  </action>
  <verify>
- All three files exist and import correctly
- `python -c "from verifier import compare_fields, compute_field_level_metrics, categorize_error"` works
- compare.py handles numeric tolerance correctly
- metrics.py computes F1 without division by zero errors
  </verify>
  <done>Core verifier logic implemented with comparison, metrics, and categorization</done>
</task>

<task type="auto">
  <name>Task 4: Implement CLI with verify-one and verify-all commands</name>
  <files>src/verifier/cli.py</files>
  <action>
Create the CLI interface with click.

**src/verifier/cli.py**:
```python
"""CLI entry point for Takeoff v2 Verifier."""
import json
from pathlib import Path
from typing import Optional
import click
import pandas as pd
import yaml

from .compare import compare_fields, flatten_dict, load_field_mapping
from .metrics import compute_field_level_metrics, compute_aggregate_metrics

def load_ground_truth_csv(csv_path: Path, mapping: dict) -> dict:
    """
    Load ground truth from CSV and convert to nested dict structure.

    The CSV has format:
    Section
    ,Field Name,Value,Unit

    We parse this and map to JSON field paths.
    """
    # Read CSV - it has a specific format with sections
    # For now, use pandas to read and parse
    df = pd.read_csv(csv_path, header=None, names=['section', 'field', 'value', 'unit'])

    # Build result dict
    result = {}
    csv_to_json = mapping.get('csv_to_json', {})

    for _, row in df.iterrows():
        field_name = str(row['field']).strip() if pd.notna(row['field']) else None
        value = row['value']

        if field_name and field_name in csv_to_json:
            json_path = csv_to_json[field_name]
            # Parse value to appropriate type
            if pd.notna(value):
                # Try to convert to number
                try:
                    if '.' in str(value):
                        value = float(value)
                    else:
                        value = int(value)
                except (ValueError, TypeError):
                    value = str(value).strip().strip('"')

                # Set in result dict using path
                keys = json_path.split('.')
                d = result
                for key in keys[:-1]:
                    d = d.setdefault(key, {})
                d[keys[-1]] = value

    return result

def load_extracted_json(json_path: Path) -> dict:
    """Load extracted JSON from file."""
    with open(json_path) as f:
        return json.load(f)

@click.group()
def cli():
    """Takeoff v2 Verifier - Evaluate extraction quality against ground truth."""
    pass

@cli.command()
@click.argument('eval_id')
@click.argument('extracted_json', type=click.Path(exists=True))
@click.option('--evals-dir', type=click.Path(exists=True), default='evals',
              help='Directory containing eval datasets')
@click.option('--output', '-o', type=click.Path(), default=None,
              help='Output file for results JSON')
def verify_one(eval_id: str, extracted_json: str, evals_dir: str, output: Optional[str]):
    """
    Run verification on a single extraction result.

    EVAL_ID: Identifier of the eval (e.g., lamb-adu)
    EXTRACTED_JSON: Path to extracted JSON file

    Example:
        verifier verify-one lamb-adu results/extracted.json
    """
    evals_path = Path(evals_dir)
    mapping = load_field_mapping()

    # Load ground truth
    gt_path = evals_path / eval_id / "ground_truth.csv"
    if not gt_path.exists():
        click.echo(f"Error: Ground truth not found at {gt_path}", err=True)
        raise SystemExit(1)

    ground_truth = load_ground_truth_csv(gt_path, mapping)

    # Load extracted JSON
    extracted = load_extracted_json(Path(extracted_json))

    # Compare
    discrepancies = compare_fields(ground_truth, extracted, mapping)

    # Flatten for counting
    gt_flat = flatten_dict(ground_truth)
    ext_flat = flatten_dict(extracted)

    # Compute metrics
    metrics = compute_field_level_metrics(
        discrepancies,
        len(gt_flat),
        len(ext_flat)
    )

    # Output results
    click.echo(f"\n{'='*60}")
    click.echo(f"Verification Results: {eval_id}")
    click.echo(f"{'='*60}")
    click.echo(f"\nMetrics:")
    click.echo(f"  Precision: {metrics['precision']:.3f}")
    click.echo(f"  Recall:    {metrics['recall']:.3f}")
    click.echo(f"  F1 Score:  {metrics['f1']:.3f}")
    click.echo(f"\n  Correct:   {metrics['correct_fields']}/{metrics['total_fields_gt']} fields")

    click.echo(f"\nError Breakdown:")
    for error_type, count in metrics['errors_by_type'].items():
        click.echo(f"  {error_type}: {count}")

    if discrepancies:
        click.echo(f"\nField-Level Discrepancies ({len(discrepancies)} total):")
        for d in discrepancies[:20]:  # Show first 20
            click.echo(f"  [{d.error_type}] {d.field_path}")
            click.echo(f"    Expected: {d.expected}")
            click.echo(f"    Actual:   {d.actual}")
        if len(discrepancies) > 20:
            click.echo(f"  ... and {len(discrepancies) - 20} more")

    # Save results if output specified
    if output:
        results = {
            'eval_id': eval_id,
            'metrics': metrics,
            'discrepancies': [
                {
                    'field_path': d.field_path,
                    'expected': d.expected,
                    'actual': d.actual,
                    'error_type': d.error_type
                }
                for d in discrepancies
            ]
        }
        Path(output).write_text(json.dumps(results, indent=2, default=str))
        click.echo(f"\nResults saved to: {output}")

@cli.command()
@click.option('--evals-dir', type=click.Path(exists=True), default='evals',
              help='Directory containing eval datasets')
@click.option('--results-subdir', default='results',
              help='Subdirectory within each eval containing extraction results')
@click.option('--output', '-o', type=click.Path(), default=None,
              help='Output file for aggregate results JSON')
def verify_all(evals_dir: str, results_subdir: str, output: Optional[str]):
    """
    Run verification on all evals and show aggregate metrics.

    Looks for extracted.json in each eval's results directory.

    Example:
        verifier verify-all
        verifier verify-all --evals-dir ./evals --output aggregate.json
    """
    evals_path = Path(evals_dir)
    mapping = load_field_mapping()

    # Load manifest
    manifest_path = evals_path / "manifest.yaml"
    if not manifest_path.exists():
        click.echo(f"Error: Manifest not found at {manifest_path}", err=True)
        raise SystemExit(1)

    with open(manifest_path) as f:
        manifest = yaml.safe_load(f)

    all_metrics = []
    results_by_eval = {}

    for eval_id in manifest.get('evals', {}).keys():
        # Find latest extraction result
        results_dir = evals_path / eval_id / results_subdir
        if not results_dir.exists():
            click.echo(f"  Skipping {eval_id}: no results directory")
            continue

        # Look for extracted.json in results dir or latest iteration
        extracted_path = results_dir / "extracted.json"
        if not extracted_path.exists():
            # Check for iteration directories
            iter_dirs = sorted([d for d in results_dir.iterdir() if d.is_dir()])
            if iter_dirs:
                extracted_path = iter_dirs[-1] / "extracted.json"

        if not extracted_path.exists():
            click.echo(f"  Skipping {eval_id}: no extracted.json found")
            continue

        # Load and compare
        gt_path = evals_path / eval_id / "ground_truth.csv"
        ground_truth = load_ground_truth_csv(gt_path, mapping)
        extracted = load_extracted_json(extracted_path)

        discrepancies = compare_fields(ground_truth, extracted, mapping)
        gt_flat = flatten_dict(ground_truth)
        ext_flat = flatten_dict(extracted)

        metrics = compute_field_level_metrics(
            discrepancies,
            len(gt_flat),
            len(ext_flat)
        )
        metrics['eval_id'] = eval_id

        all_metrics.append(metrics)
        results_by_eval[eval_id] = {
            'metrics': metrics,
            'discrepancy_count': len(discrepancies)
        }

    if not all_metrics:
        click.echo("\nNo evaluations found with extraction results.")
        click.echo("Run extraction first, then verify.")
        raise SystemExit(1)

    # Compute aggregate
    aggregate = compute_aggregate_metrics(all_metrics)

    # Output results
    click.echo(f"\n{'='*60}")
    click.echo("Aggregate Verification Results")
    click.echo(f"{'='*60}")
    click.echo(f"\nEvaluated: {len(all_metrics)}/{len(manifest.get('evals', {}))} evals")
    click.echo(f"\nAggregate Metrics (Macro-Average):")
    click.echo(f"  Precision: {aggregate['precision']:.3f}")
    click.echo(f"  Recall:    {aggregate['recall']:.3f}")
    click.echo(f"  F1 Score:  {aggregate['f1']:.3f}")

    click.echo(f"\nPer-Eval Breakdown:")
    click.echo(f"  {'Eval':<25} {'P':>8} {'R':>8} {'F1':>8} {'Errors':>8}")
    click.echo(f"  {'-'*25} {'-'*8} {'-'*8} {'-'*8} {'-'*8}")
    for m in all_metrics:
        click.echo(f"  {m['eval_id']:<25} {m['precision']:>8.3f} {m['recall']:>8.3f} {m['f1']:>8.3f} {sum(m['errors_by_type'].values()):>8}")

    # Save results if output specified
    if output:
        output_data = {
            'aggregate': aggregate,
            'by_eval': results_by_eval
        }
        Path(output).write_text(json.dumps(output_data, indent=2, default=str))
        click.echo(f"\nResults saved to: {output}")

if __name__ == '__main__':
    cli()
```
  </action>
  <verify>
- CLI works: `python -m verifier --help` shows commands
- verify-one command: `python -m verifier verify-one --help` shows options
- verify-all command: `python -m verifier verify-all --help` shows options
- Test with mock data if available
  </verify>
  <done>CLI implemented with verify-one and verify-all commands</done>
</task>

</tasks>

<verification>
Run these checks to verify the verifier infrastructure:

```bash
# Check package installs
pip install -e ".[dev]"

# Check imports
python -c "from verifier import compare_fields, compute_field_level_metrics, categorize_error; print('Imports OK')"

# Check CLI help
python -m verifier --help
python -m verifier verify-one --help
python -m verifier verify-all --help

# Check schema imports
python -c "from schemas.building_spec import BuildingSpec; print('Schema OK')"

# Check field mapping loads
python -c "from verifier.compare import load_field_mapping; m = load_field_mapping(); print(f'Mapped {len(m.get(\"csv_to_json\", {}))} fields')"
```
</verification>

<success_criteria>
1. `pip install -e ".[dev]"` succeeds
2. `python -m verifier verify-one <eval_id> <json>` runs and outputs P/R/F1
3. `python -m verifier verify-all` runs and outputs aggregate metrics
4. Verifier correctly categorizes errors (omission, hallucination, format_error, wrong_value)
5. Field mapping correctly maps CSV columns to JSON paths
6. Numeric comparison uses tolerance (±0.5% or ±0.01)
7. String comparison is case-insensitive with trimming
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
