---
phase: 01-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-02"]
files_modified:
  - src/verifier/report.py
  - src/verifier/persistence.py
  - src/verifier/templates/eval-report.html.j2
  - src/verifier/cli.py
autonomous: true

must_haves:
  truths:
    - "Verifier generates HTML report showing metrics, error breakdown, and field discrepancies"
    - "Evaluation results persist to iteration-numbered directories"
    - "User can track F1 progression across iterations"
    - "HTML report is readable in browser with color-coded error types"
  artifacts:
    - path: "src/verifier/report.py"
      provides: "HTML report generation"
      exports: ["EvalReport", "generate_html_report"]
    - path: "src/verifier/persistence.py"
      provides: "Iteration tracking and result storage"
      exports: ["EvalStore", "save_evaluation", "get_next_iteration"]
    - path: "src/verifier/templates/eval-report.html.j2"
      provides: "Jinja2 HTML template for eval reports"
      contains: ["precision", "recall", "F1", "discrepancies"]
  key_links:
    - from: "src/verifier/cli.py"
      to: "src/verifier/report.py"
      via: "import for HTML generation"
      pattern: "from.*report import"
    - from: "src/verifier/cli.py"
      to: "src/verifier/persistence.py"
      via: "import for result storage"
      pattern: "from.*persistence import"
    - from: "src/verifier/report.py"
      to: "src/verifier/templates/eval-report.html.j2"
      via: "Jinja2 template loading"
      pattern: "get_template.*eval-report"
---

<objective>
Add HTML report generation and iteration persistence to the verifier, enabling visual inspection of results and tracking improvement across iterations.

Purpose: Enable debugging extraction issues through visual reports and measure improvement over time. Reports are critical for understanding what's failing and whether changes help.

Output: HTML reports in evals/{eval_id}/results/iteration-NNN/ with metrics, error breakdown, and field-level discrepancies. Aggregate tracking in aggregate.json.
</objective>

<execution_context>
@/Users/Andrew/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Andrew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

This plan depends on 01-02 which provides the core verifier logic (compare.py, metrics.py, categorize.py). We extend the CLI to generate HTML reports and persist results.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create HTML report template</name>
  <files>src/verifier/templates/eval-report.html.j2</files>
  <action>
Create the Jinja2 HTML template for evaluation reports.

```bash
mkdir -p src/verifier/templates
```

**src/verifier/templates/eval-report.html.j2**:
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Report: {{ eval_id }} - Iteration {{ iteration }}</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 30px;
            margin-bottom: 20px;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        .metadata {
            background: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .metadata span {
            margin-right: 30px;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .metric-card.good {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }
        .metric-card.bad {
            background: linear-gradient(135deg, #eb3349 0%, #f45c43 100%);
        }
        .metric-value {
            font-size: 2.5em;
            font-weight: bold;
        }
        .metric-label {
            font-size: 0.9em;
            opacity: 0.9;
        }
        .error-breakdown {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .error-type {
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid;
        }
        .error-type.omission {
            background: #fff3cd;
            border-color: #ffc107;
        }
        .error-type.hallucination {
            background: #f8d7da;
            border-color: #dc3545;
        }
        .error-type.format_error {
            background: #e2d5f1;
            border-color: #6f42c1;
        }
        .error-type.wrong_value {
            background: #cce5ff;
            border-color: #007bff;
        }
        .error-count {
            font-size: 1.8em;
            font-weight: bold;
        }
        .error-label {
            font-size: 0.85em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
        }
        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background: #34495e;
            color: white;
            font-weight: 500;
            position: sticky;
            top: 0;
        }
        tr:hover {
            background: #f8f9fa;
        }
        tr.omission td { background: rgba(255, 193, 7, 0.1); }
        tr.hallucination td { background: rgba(220, 53, 69, 0.1); }
        tr.format_error td { background: rgba(111, 66, 193, 0.1); }
        tr.wrong_value td { background: rgba(0, 123, 255, 0.1); }
        .field-path {
            font-family: "SF Mono", Monaco, "Courier New", monospace;
            font-size: 0.85em;
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
        }
        .value {
            max-width: 200px;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }
        .badge {
            display: inline-block;
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 0.75em;
            font-weight: 500;
            text-transform: uppercase;
        }
        .badge.omission { background: #ffc107; color: #856404; }
        .badge.hallucination { background: #dc3545; color: white; }
        .badge.format_error { background: #6f42c1; color: white; }
        .badge.wrong_value { background: #007bff; color: white; }
        .filter-buttons {
            margin: 20px 0;
        }
        .filter-btn {
            padding: 8px 16px;
            margin-right: 8px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 0.9em;
            transition: opacity 0.2s;
        }
        .filter-btn:hover { opacity: 0.8; }
        .filter-btn.active { box-shadow: 0 0 0 3px rgba(0,0,0,0.2); }
        .filter-btn.omission { background: #ffc107; }
        .filter-btn.hallucination { background: #dc3545; color: white; }
        .filter-btn.format_error { background: #6f42c1; color: white; }
        .filter-btn.wrong_value { background: #007bff; color: white; }
        .filter-btn.all { background: #6c757d; color: white; }
        .hidden { display: none; }
        footer {
            text-align: center;
            padding: 20px;
            color: #666;
            font-size: 0.85em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Evaluation Report: {{ eval_id }}</h1>
        <div class="metadata">
            <span><strong>Iteration:</strong> {{ iteration }}</span>
            <span><strong>Timestamp:</strong> {{ timestamp }}</span>
            <span><strong>Total Fields:</strong> {{ total_fields }}</span>
        </div>

        <h2>Metrics</h2>
        <div class="metrics-grid">
            <div class="metric-card {% if metrics.precision >= 0.8 %}good{% elif metrics.precision < 0.5 %}bad{% endif %}">
                <div class="metric-value">{{ "%.1f"|format(metrics.precision * 100) }}%</div>
                <div class="metric-label">Precision</div>
            </div>
            <div class="metric-card {% if metrics.recall >= 0.8 %}good{% elif metrics.recall < 0.5 %}bad{% endif %}">
                <div class="metric-value">{{ "%.1f"|format(metrics.recall * 100) }}%</div>
                <div class="metric-label">Recall</div>
            </div>
            <div class="metric-card {% if metrics.f1 >= 0.8 %}good{% elif metrics.f1 < 0.5 %}bad{% endif %}">
                <div class="metric-value">{{ "%.1f"|format(metrics.f1 * 100) }}%</div>
                <div class="metric-label">F1 Score</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ correct_fields }}/{{ total_fields }}</div>
                <div class="metric-label">Correct Fields</div>
            </div>
        </div>

        <h2>Error Breakdown</h2>
        <div class="error-breakdown">
            {% for error_type, count in errors_by_type.items() %}
            <div class="error-type {{ error_type }}">
                <div class="error-count">{{ count }}</div>
                <div class="error-label">{{ error_type | replace('_', ' ') }}</div>
            </div>
            {% endfor %}
        </div>

        <h2>Field-Level Discrepancies</h2>
        {% if discrepancies %}
        <div class="filter-buttons">
            <button class="filter-btn all active" onclick="filterTable('all')">All ({{ discrepancies|length }})</button>
            {% for error_type, count in errors_by_type.items() %}
            <button class="filter-btn {{ error_type }}" onclick="filterTable('{{ error_type }}')">
                {{ error_type | replace('_', ' ') | title }} ({{ count }})
            </button>
            {% endfor %}
        </div>

        <table id="discrepancies-table">
            <thead>
                <tr>
                    <th>Field Path</th>
                    <th>Expected</th>
                    <th>Actual</th>
                    <th>Error Type</th>
                </tr>
            </thead>
            <tbody>
                {% for d in discrepancies %}
                <tr class="{{ d.error_type }}" data-type="{{ d.error_type }}">
                    <td><span class="field-path">{{ d.field_path }}</span></td>
                    <td class="value" title="{{ d.expected }}">{{ d.expected if d.expected is not none else '(missing)' }}</td>
                    <td class="value" title="{{ d.actual }}">{{ d.actual if d.actual is not none else '(missing)' }}</td>
                    <td><span class="badge {{ d.error_type }}">{{ d.error_type | replace('_', ' ') }}</span></td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
        {% else %}
        <p>No discrepancies found. Perfect extraction!</p>
        {% endif %}
    </div>

    <footer>
        Generated by Takeoff v2 Verifier | Iteration {{ iteration }}
    </footer>

    <script>
        function filterTable(type) {
            const rows = document.querySelectorAll('#discrepancies-table tbody tr');
            const buttons = document.querySelectorAll('.filter-btn');

            buttons.forEach(btn => btn.classList.remove('active'));
            document.querySelector(`.filter-btn.${type === 'all' ? 'all' : type}`).classList.add('active');

            rows.forEach(row => {
                if (type === 'all' || row.dataset.type === type) {
                    row.classList.remove('hidden');
                } else {
                    row.classList.add('hidden');
                }
            });
        }
    </script>
</body>
</html>
```
  </action>
  <verify>
- Template file exists at src/verifier/templates/eval-report.html.j2
- Template contains all required sections: metrics, error breakdown, discrepancies table
- Template uses Jinja2 syntax correctly ({{ }}, {% %})
  </verify>
  <done>HTML report template created with metrics, error breakdown, and filterable discrepancy table</done>
</task>

<task type="auto">
  <name>Task 2: Implement report generation and persistence</name>
  <files>
src/verifier/report.py
src/verifier/persistence.py
  </files>
  <action>
Create the report generation and persistence modules.

**src/verifier/report.py**:
```python
"""HTML report generation for evaluation results."""
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from jinja2 import Environment, FileSystemLoader, select_autoescape

from .compare import FieldDiscrepancy

class EvalReport:
    """Generates HTML evaluation reports."""

    def __init__(
        self,
        eval_id: str,
        iteration: int,
        discrepancies: List[FieldDiscrepancy],
        metrics: Dict[str, Any]
    ):
        self.eval_id = eval_id
        self.iteration = iteration
        self.discrepancies = discrepancies
        self.metrics = metrics
        self.timestamp = datetime.now().isoformat()

    def _get_template_env(self) -> Environment:
        """Get Jinja2 environment with template directory."""
        template_dir = Path(__file__).parent / "templates"
        return Environment(
            loader=FileSystemLoader(template_dir),
            autoescape=select_autoescape(['html', 'xml'])
        )

    def _group_errors_by_type(self) -> Dict[str, int]:
        """Group discrepancies by error type and count."""
        errors_by_type = {}
        for d in self.discrepancies:
            errors_by_type[d.error_type] = errors_by_type.get(d.error_type, 0) + 1
        return errors_by_type

    def generate_html(self, output_path: Path) -> None:
        """
        Generate HTML report and write to file.

        Args:
            output_path: Path to write HTML file
        """
        env = self._get_template_env()
        template = env.get_template("eval-report.html.j2")

        errors_by_type = self._group_errors_by_type()

        # Convert discrepancies to dicts for template
        discrepancy_dicts = [
            {
                "field_path": d.field_path,
                "expected": d.expected,
                "actual": d.actual,
                "error_type": d.error_type
            }
            for d in self.discrepancies
        ]

        html = template.render(
            eval_id=self.eval_id,
            iteration=self.iteration,
            timestamp=self.timestamp,
            metrics=self.metrics,
            discrepancies=discrepancy_dicts,
            errors_by_type=errors_by_type,
            total_fields=self.metrics.get('total_fields_gt', 0),
            correct_fields=self.metrics.get('correct_fields', 0)
        )

        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(html)

    def to_dict(self) -> Dict[str, Any]:
        """Convert report to dictionary for JSON serialization."""
        return {
            "eval_id": self.eval_id,
            "iteration": self.iteration,
            "timestamp": self.timestamp,
            "metrics": self.metrics,
            "errors_by_type": self._group_errors_by_type(),
            "discrepancies": [
                {
                    "field_path": d.field_path,
                    "expected": d.expected,
                    "actual": d.actual,
                    "error_type": d.error_type
                }
                for d in self.discrepancies
            ]
        }


def generate_html_report(
    eval_id: str,
    iteration: int,
    discrepancies: List[FieldDiscrepancy],
    metrics: Dict[str, Any],
    output_path: Path
) -> EvalReport:
    """
    Convenience function to generate HTML report.

    Returns the EvalReport object for further use.
    """
    report = EvalReport(eval_id, iteration, discrepancies, metrics)
    report.generate_html(output_path)
    return report
```

**src/verifier/persistence.py**:
```python
"""Persistence layer for evaluation results and iteration tracking."""
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

class EvalStore:
    """
    Manages evaluation result storage and iteration tracking.

    Directory structure:
    evals/
    └── {eval_id}/
        └── results/
            ├── iteration-001/
            │   ├── extracted.json
            │   ├── eval-results.json
            │   └── eval-report.html
            ├── iteration-002/
            │   └── ...
            └── aggregate.json
    """

    def __init__(self, evals_dir: Path = None):
        self.evals_dir = evals_dir or Path("evals")

    def get_next_iteration(self, eval_id: str) -> int:
        """
        Get the next iteration number for this eval.

        Returns 1 if no iterations exist yet.
        """
        results_dir = self.evals_dir / eval_id / "results"
        if not results_dir.exists():
            return 1

        existing = [d.name for d in results_dir.iterdir() if d.is_dir()]
        iterations = []
        for d in existing:
            if d.startswith('iteration-'):
                try:
                    iterations.append(int(d.split('-')[1]))
                except (ValueError, IndexError):
                    pass

        return max(iterations, default=0) + 1

    def get_iteration_dir(self, eval_id: str, iteration: int) -> Path:
        """Get path to iteration directory."""
        return self.evals_dir / eval_id / "results" / f"iteration-{iteration:03d}"

    def save_evaluation(
        self,
        eval_id: str,
        iteration: int,
        extracted: Dict[str, Any],
        results: Dict[str, Any],
        report_html: str
    ) -> Path:
        """
        Save all evaluation artifacts for this iteration.

        Args:
            eval_id: Evaluation identifier
            iteration: Iteration number
            extracted: Raw extraction output
            results: Evaluation results (metrics, discrepancies)
            report_html: Generated HTML report content

        Returns:
            Path to the iteration directory
        """
        iter_dir = self.get_iteration_dir(eval_id, iteration)
        iter_dir.mkdir(parents=True, exist_ok=True)

        # Save extracted JSON
        extracted_path = iter_dir / "extracted.json"
        extracted_path.write_text(json.dumps(extracted, indent=2, default=str))

        # Add metadata to results
        results['metadata'] = {
            'eval_id': eval_id,
            'iteration': iteration,
            'timestamp': datetime.now().isoformat()
        }

        # Save eval results
        results_path = iter_dir / "eval-results.json"
        results_path.write_text(json.dumps(results, indent=2, default=str))

        # Save HTML report
        report_path = iter_dir / "eval-report.html"
        report_path.write_text(report_html)

        # Update aggregate metrics
        self._update_aggregate(eval_id, iteration, results)

        return iter_dir

    def _update_aggregate(
        self,
        eval_id: str,
        iteration: int,
        results: Dict[str, Any]
    ) -> None:
        """Update aggregate.json with metrics from this iteration."""
        agg_path = self.evals_dir / eval_id / "results" / "aggregate.json"

        if agg_path.exists():
            aggregate = json.loads(agg_path.read_text())
        else:
            aggregate = {'iterations': []}

        # Remove existing entry for this iteration if present
        aggregate['iterations'] = [
            i for i in aggregate['iterations']
            if i.get('iteration') != iteration
        ]

        # Add new entry
        metrics = results.get('metrics', {})
        aggregate['iterations'].append({
            'iteration': iteration,
            'timestamp': results.get('metadata', {}).get('timestamp', datetime.now().isoformat()),
            'precision': metrics.get('precision', 0),
            'recall': metrics.get('recall', 0),
            'f1': metrics.get('f1', 0),
            'total_errors': sum(metrics.get('errors_by_type', {}).values()),
            'errors_by_type': metrics.get('errors_by_type', {})
        })

        # Sort by iteration number
        aggregate['iterations'].sort(key=lambda x: x['iteration'])

        agg_path.write_text(json.dumps(aggregate, indent=2))

    def get_iteration_history(self, eval_id: str) -> List[Dict[str, Any]]:
        """
        Get history of all iterations for this eval.

        Returns list of iteration summaries from aggregate.json.
        """
        agg_path = self.evals_dir / eval_id / "results" / "aggregate.json"

        if not agg_path.exists():
            return []

        aggregate = json.loads(agg_path.read_text())
        return aggregate.get('iterations', [])

    def load_iteration_results(
        self,
        eval_id: str,
        iteration: int
    ) -> Optional[Dict[str, Any]]:
        """Load full results for a specific iteration."""
        results_path = self.get_iteration_dir(eval_id, iteration) / "eval-results.json"

        if not results_path.exists():
            return None

        return json.loads(results_path.read_text())


def save_evaluation(
    eval_id: str,
    iteration: int,
    extracted: Dict[str, Any],
    results: Dict[str, Any],
    report_html: str,
    evals_dir: Path = None
) -> Path:
    """Convenience function for saving evaluation results."""
    store = EvalStore(evals_dir)
    return store.save_evaluation(eval_id, iteration, extracted, results, report_html)


def get_next_iteration(eval_id: str, evals_dir: Path = None) -> int:
    """Convenience function for getting next iteration number."""
    store = EvalStore(evals_dir)
    return store.get_next_iteration(eval_id)
```
  </action>
  <verify>
- Both files exist and import correctly
- `python -c "from verifier.report import EvalReport, generate_html_report"` works
- `python -c "from verifier.persistence import EvalStore, save_evaluation, get_next_iteration"` works
  </verify>
  <done>Report generation and persistence modules implemented</done>
</task>

<task type="auto">
  <name>Task 3: Update CLI to use reporting and persistence</name>
  <files>src/verifier/cli.py</files>
  <action>
Update the CLI to integrate HTML report generation and result persistence.

Modify **src/verifier/cli.py** to:

1. Add imports at top:
```python
from .report import EvalReport, generate_html_report
from .persistence import EvalStore, get_next_iteration
```

2. Update `verify_one` command to:
- Generate HTML report after computing metrics
- Save results to iteration directory if `--save` flag is provided
- Open report in browser if `--open` flag is provided

3. Update `verify_all` command to:
- Generate per-eval HTML reports
- Generate aggregate summary HTML
- Save all results if `--save` flag is provided

Add these options and update the implementation:

```python
@cli.command()
@click.argument('eval_id')
@click.argument('extracted_json', type=click.Path(exists=True))
@click.option('--evals-dir', type=click.Path(exists=True), default='evals',
              help='Directory containing eval datasets')
@click.option('--output', '-o', type=click.Path(), default=None,
              help='Output file for results JSON')
@click.option('--save/--no-save', default=True,
              help='Save results to iteration directory')
@click.option('--open-report', is_flag=True,
              help='Open HTML report in browser after generation')
def verify_one(eval_id: str, extracted_json: str, evals_dir: str,
               output: Optional[str], save: bool, open_report: bool):
    # ... existing loading code ...

    # After computing metrics, add:
    store = EvalStore(Path(evals_dir))
    iteration = store.get_next_iteration(eval_id) if save else 0

    # Generate report
    report = EvalReport(eval_id, iteration, discrepancies, metrics)

    if save:
        iter_dir = store.get_iteration_dir(eval_id, iteration)
        report_path = iter_dir / "eval-report.html"
        report.generate_html(report_path)

        # Save all artifacts
        store.save_evaluation(
            eval_id=eval_id,
            iteration=iteration,
            extracted=extracted,
            results=report.to_dict(),
            report_html=report_path.read_text()
        )

        click.echo(f"\nResults saved to: {iter_dir}")
        click.echo(f"  HTML Report: {report_path}")

        if open_report:
            import webbrowser
            webbrowser.open(f"file://{report_path.absolute()}")
    else:
        # Generate to temp location for viewing
        if open_report:
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.html', delete=False) as f:
                temp_path = Path(f.name)
            report.generate_html(temp_path)
            import webbrowser
            webbrowser.open(f"file://{temp_path.absolute()}")
```

Also add a new command to view iteration history:

```python
@cli.command()
@click.argument('eval_id')
@click.option('--evals-dir', type=click.Path(exists=True), default='evals')
def history(eval_id: str, evals_dir: str):
    """Show F1 progression across iterations for an eval."""
    store = EvalStore(Path(evals_dir))
    iterations = store.get_iteration_history(eval_id)

    if not iterations:
        click.echo(f"No iteration history for {eval_id}")
        return

    click.echo(f"\nIteration History: {eval_id}")
    click.echo(f"{'Iter':<6} {'F1':>8} {'P':>8} {'R':>8} {'Errors':>8} {'Timestamp':<20}")
    click.echo("-" * 60)

    for i in iterations:
        click.echo(
            f"{i['iteration']:<6} "
            f"{i['f1']:>8.3f} "
            f"{i['precision']:>8.3f} "
            f"{i['recall']:>8.3f} "
            f"{i['total_errors']:>8} "
            f"{i['timestamp'][:19]:<20}"
        )
```

Update the `__init__.py` to export the new modules:

```python
# src/verifier/__init__.py
from .compare import compare_fields, FieldDiscrepancy
from .metrics import compute_field_level_metrics
from .categorize import categorize_error
from .report import EvalReport, generate_html_report
from .persistence import EvalStore, save_evaluation, get_next_iteration

__all__ = [
    "compare_fields",
    "FieldDiscrepancy",
    "compute_field_level_metrics",
    "categorize_error",
    "EvalReport",
    "generate_html_report",
    "EvalStore",
    "save_evaluation",
    "get_next_iteration",
]
```
  </action>
  <verify>
- CLI includes new options: --save, --open-report
- `python -m verifier verify-one --help` shows new options
- `python -m verifier history --help` works
- Running verify-one with --save creates iteration directory with HTML report
  </verify>
  <done>CLI updated with HTML reporting, persistence, and history viewing</done>
</task>

</tasks>

<verification>
Run these checks to verify the reporting and persistence system:

```bash
# Check template exists
ls -la src/verifier/templates/eval-report.html.j2

# Check imports work
python -c "from verifier.report import EvalReport; print('Report OK')"
python -c "from verifier.persistence import EvalStore; print('Persistence OK')"

# Check CLI has new commands
python -m verifier --help
python -m verifier verify-one --help  # Should show --save, --open-report
python -m verifier history --help

# Test with mock data (create a simple test)
# Create test extracted.json and run verify-one with --save
```
</verification>

<success_criteria>
1. HTML report template exists with metrics, error breakdown, and discrepancy table
2. `verify-one --save` creates iteration directory with extracted.json, eval-results.json, eval-report.html
3. HTML report renders correctly in browser with color-coded error types
4. `verify-all --save` generates reports for all evals
5. `history` command shows F1 progression across iterations
6. aggregate.json tracks metrics across all iterations for each eval
7. Reports limit discrepancies to prevent browser performance issues (handled by pagination in template)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
